# -*- coding: utf-8 -*-
"""1.모델서빙_거래량총합계_반환_서비스_박도현.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YFGPb1L2GRm3FMktxZJX-QBmI4jPJMFa
"""

# prompt: 구글드라이브 연동 해줘

# from google.colab import drive
# drive.mount('/content/gdrive')

### 코랩의 기본 라이브러리가 아님
!pip install uvicorn fastapi
!pip install nest-asyncio pyngrok

### DB관련 라이브러리
!pip install pymysql
!pip install sqlalchemy
!pip install mysql-connector-python

# AI관련 라이브러리
!pip install -U pandas==2.2.2 numpy==2.0.2 scipy==1.14.1 accelerate==1.6.0 peft==0.15.2 bitsandbytes==0.45.5 transformers==4.51.3 trl==0.16.1 datasets==3.5.0 tensorboard==2.19.0

### 필요 라이브러리 설정
# 서버 관리용 fastapi 의존 라이브러리/포트설정?
import uvicorn

# fast api 라이브러리
from fastapi import FastAPI

# 머신러닝 모델 관리용 라이브러리
# import pickle

# 데이터프레임 및 수 처리 라이브러리
import pandas as pd
import numpy as np

# 인터페이스 데이터 관리를 위한 라이브러리
from pydantic import BaseModel

# # DB관련
# import pandas as pd
# from sqlalchemy import create_engine, inspect


# AI관련
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging
)
from transformers import AutoConfig,AutoModel
import torch
from peft import PeftModel, PeftConfig

# 허깅페이스
import huggingface_hub

### CORS: 웹 브라우저 보안 정책 중 하나/모든 데이터를 신뢰하겠다는 설정을 해줘야함, 포트랑 비슷
from fastapi.middleware.cors import CORSMiddleware
origins = ["*"]

app = FastAPI(title="DB_TOTAL_SUM_API")

# CORS 미들웨어 추가
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # 모든 origin 허용, 사이즈
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"], # 유저에이전트, 제이슨타입 등
)

huggingface_hub.login("") # 3.0b 라마

### 내꺼 모델 불러오기
base_model = "imaginaryi/TranslateEnglishToKorean_3.8b_model" # 3-8b 라마 내꺼
### 베이스모델 불러오기
myModel= AutoModelForCausalLM.from_pretrained(
base_model,
low_cpu_mem_usage=True,
return_dict=True,
torch_dtype=torch.float16,
device_map= "auto" # T4 GPU 사용 시
# device_map= {"": 0} # L4 이상 GRU 사용시
)
### 토크나이저 불러오기
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.pad_token= tokenizer.eos_token
tokenizer.padding_side= "right"

### text-generation 작업을 위한 파이프라인 객체 생성
pipe = pipeline(
    # 작업 유형: 텍스트 생성
    task="text-generation",
    # 사용할 모델
    model=myModel,
    # 모델의 토크나이저
    tokenizer=tokenizer,
    # 모델에 전달할 추가 인자: float16 데이터 타입을 사용하여 모델을 로드 ("bfloat16"으로 학습했다면 bfloat16 사용)
    model_kwargs={"torch_dtype": torch.float16},
    # 입력 텍스트가 너무 길 경우 잘라내기
    truncation=True
)

def extract_response_llama3(question):
    # 사용자 질문을 포함하는 메시지 리스트를 생성
    messages = [
        {"role": "system", "content": ""},       # 시스템 메시지: 일반적으로 시스템의 지침이나 상태값 저장
        {"role": "user", "content": question},   # 사용자 메시지: 사용자 질문을 포함
    ]
    # 메시지를 토크나이저를 사용하여 모델의 입력 형식에 맞게 변환
    prompt = pipe.tokenizer.apply_chat_template(
        messages,                             # 메시지 리스트
        tokenize=False,                       # 토크나이즈를 하지 않음
        add_generation_prompt=True            # 텍스트 생성을 위한 프롬프트 추가
    )
    # 텍스트 생성 종료 토큰 ID 목록
    terminators = [
        pipe.tokenizer.eos_token_id,                        # End Of Sequence 토큰 ID
        pipe.tokenizer.convert_tokens_to_ids("<|eot_id|>")  # 사용자 정의 종료 토큰 ID
    ]
    outputs = pipe(
        prompt,                          # 생성할 프롬프트
        max_new_tokens=256,              # 생성할 최대 토큰 수
        eos_token_id=terminators,        # 텍스트 생성 종료를 위한 토큰 ID
        do_sample=False,                  # 샘플링을 사용하여 다음 토큰(텍스트) 생성 (True 시 토큰 무작위 선택 하여 창의성  높임, False 시 창의성 낮춤)
        temperature=0.1,                 # 샘플링의 온도 설정: 낮은 온도는 자유도를 주지 않음 (0보다 크고1보다 작아야함)
        # top_p=0.9,                       # 상위 90%의 확률을 가진 토큰중에서만 무작위로 선택 do_sample true인 경우에만 설정
        num_return_sequences=1           # 생성할 시퀀스의 수: 한 개만 생성
    )
    generated_text = outputs[0]['generated_text']        # 생성된 텍스트를 추출
    response_lines = generated_text.strip().split('\n')  # 텍스트를 줄 단위로 분리
    meaningful_response = response_lines[-1]             # 마지막 줄을 응답으로 선택
    return meaningful_response

# ### 내가 만든 질문 모음집 확인용
# # question = "The scattered signal is selected by the receiving optical channel and then provided to one or more detectors. 한글로 번역해봐"
# # question = "Abdominal aspirate revealed delicately filamentous Gram positive bacilli but failed to isolate the bacteria 한글로 번역해봐"
# # question = "Through computerized cognitive rehabilitation, cognitive function was significantly improved in patients with mild cognitive impairment and Alzheimer's disease. 한글로 번역주실레요?"
# question = "Through computerized cognitive rehabilitation, cognitive function was significantly improved in patients with mild cognitive impairment and Alzheimer's disease. 한글로 번역해봐"
# response = extract_response_llama3(question)
# print(response)

### 인터페이스 데이터 정의
class InDataset(BaseModel):
    question:str # 인바운드, 아웃바운드 중 정의

### 예측용
@app.post("/predictLLAMA38B", status_code=200)
async def predict_tf(x: InDataset):
    print(x)
    question = x.question
    ### 입력값 활용 로직 구현 ###
    response = extract_response_llama3(question)
    print(response)
    return {"prediction": response }

@app.get('/')
async def root():
    return {"message": "online"}

### 윈도우 서버오픈
!pip install nest-asynciopyngrok
import nest_asyncio
from pyngrok import ngrok
# import uvicorn

auth_token= "2ucBRiLx6RH8xTgb4oDB4x5TeVU_74VTWRTpyUanxyWQbr4Zs" # 나의 ngrok 토큰
ngrok.set_auth_token(auth_token)
ngrokTunnel= ngrok.connect(9999)
print("공용 URL", ngrokTunnel.public_url)
nest_asyncio.apply()
uvicorn.run(app, port=9999)

# ### 리눅스 서버오픈/내꺼
# # import uvicorn
# if __name__ == "__main__":
#     uvicorn.run("pythonCode:app", host="0.0.0.0", port=9909, log_level="debug",
#         proxy_headers=True, reload=True)